{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SEED = 12345\n",
    "np.random.seed(CUSTOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     94824\n",
       "unique       35\n",
       "top        zero\n",
       "freq       3634\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data\n",
    "feat = np.load(\"feat.npy\")\n",
    "path = np.load(\"path.npy\")\n",
    "train = pd.read_csv(\"train.csv\")  #load as pandas dataframe\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#extract the target values (y)\n",
    "words = train['word']\n",
    "words.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "Train Shape    : (94824, 2)\n",
      "Train Dimension: 2\n",
      "                                                path   word\n",
      "count                                          94824  94824\n",
      "unique                                         94824     35\n",
      "top     1827be43f904639d2d190e4da9b8d08ce300316b.wav   zero\n",
      "freq                                               1   3634\n",
      "\n",
      "Test Set\n",
      "Test Shape    : (11005, 1)\n",
      "Test Dimension: 2\n",
      "                                                path\n",
      "count                                          11005\n",
      "unique                                         11005\n",
      "top     964bbb052da81db755a302d7555b0f11bf489972.wav\n",
      "freq                                               1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set\")\n",
    "print(\"Train Shape    :\", train.shape)\n",
    "print(\"Train Dimension:\", train.ndim)\n",
    "print(train.describe())\n",
    "print()\n",
    "print(\"Test Set\")\n",
    "print(\"Test Shape    :\", test.shape)\n",
    "print(\"Test Dimension:\", test.ndim)\n",
    "print(test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Set\n",
      "Path Shape    : (105835,)\n",
      "Path Dimension: 1\n",
      "Feat Set\n",
      "Feat Shape    : (105835,)\n",
      "Feat Dimension: 1\n",
      "feat[0] size  : 1287\n",
      "feat[0] shape : (99, 13)\n",
      "Each element has 13 colums representing MFCC coefficients\n",
      "feat[0] ndim  : 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Path Set\")\n",
    "print(\"Path Shape    :\", path.shape)\n",
    "print(\"Path Dimension:\", path.ndim)\n",
    "print(\"Feat Set\")\n",
    "print(\"Feat Shape    :\", feat.shape)\n",
    "print(\"Feat Dimension:\", feat.ndim)\n",
    "print(\"feat[0] size  :\", feat[0].size)\n",
    "print(\"feat[0] shape :\", feat[0].shape)\n",
    "print(\"Each element has {} colums representing MFCC coefficients\".format(feat[0].shape[1]))\n",
    "print(\"feat[0] ndim  :\", feat[0].ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we have to find the right features that are used in the training set and test set\n",
    "def find_indices(portion):\n",
    "    \"\"\"\n",
    "    This function find the index number of every wav path \n",
    "    from the given source path data set\n",
    "    \n",
    "    input : data set which has paths\n",
    "            \n",
    "    return: index list of data\n",
    "    \"\"\"\n",
    "    indexlist = []\n",
    "\n",
    "    for index, portionpath in enumerate(portion['path']):   #enumerate trought the training set\n",
    "        indexlist.append(np.where(path == portionpath))   #for every path in the train.csv, find the index in path.npy\n",
    "        print(index) #(this is to keep track if you want to know how long its going to take)\n",
    "    \n",
    "    #to extract the features from the feat.npy: \n",
    "    #for some reason i only could get it to work with integers as index, so extract those from the trainindex list\n",
    "    number_indexlist = []\n",
    "    for index in indexlist:\n",
    "        number_indexlist.append(index[0][0])\n",
    "    \n",
    "    return number_indexlist\n",
    "\n",
    "def create_feature_array(indexlist):\n",
    "    \"\"\"\n",
    "    This function extract feature values from the given \n",
    "    source feature data set according to given index numbers\n",
    "    \n",
    "    input : index list of .wav files\n",
    "            data set of MFCC features\n",
    "    return: array of features for the given indeces\n",
    "    \"\"\"\n",
    "    #create empty array, dtype = object because the features have different shapes\n",
    "    features = np.zeros((len(indexlist)), dtype = object)\n",
    "    \n",
    "    for i in range(len(indexlist)):\n",
    "        features[i] = feat[indexlist[i]]\n",
    "        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list with indices for the train and the test data sets\n",
    "train_index = find_indices(train)\n",
    "test_index =  find_indices(test)\n",
    "\n",
    "# creating new numpy arrays with only train or test features\n",
    "training_features = create_feature_array(train_index)\n",
    "test_features =     create_feature_array(test_index)\n",
    "\n",
    "# saving these numpy arrays for convenience for later use if it's needed\n",
    "np.save(\"training_features.npy\", training_features)\n",
    "np.save(\"test_features.npy\", test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the right train and test features if needed to run the code again\n",
    "training_features = np.load(\"training_features.npy\")\n",
    "test_features = np.load(\"test_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94824,), (11005,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to reshape the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros(features):\n",
    "    \"\"\"make every feature the same shape by adding rows with zeros to smaller features\"\"\"\n",
    "    fixed_shape = (99, 13)\n",
    "\n",
    "    for i in range(features.size):\n",
    "        \n",
    "        if features[i].shape != fixed_shape: #if a feature has different shape than (99,13)\n",
    "\n",
    "            rows = 99 - features[i].shape[0]  #calculate how many rows with zeros we need to add\n",
    "            zeros = np.zeros(shape=(rows, 13)) #create zero array with the rows\n",
    "\n",
    "            features[i] = np.vstack((features[i], zeros))  #add zero array to bottom of existing array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3d(features):\n",
    "    array = np.zeros(shape=(features.shape[0], 99, 13), dtype = float)\n",
    "\n",
    "    for i in range(features.size):\n",
    "        for j in range(99):\n",
    "            for k in range(13):\n",
    "\n",
    "                array[i, j, k] = features[i][j,k]\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to the same shape\n",
    "add_zeros(training_features)\n",
    "add_zeros(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_3d = make_3d(training_features)\n",
    "test_feat_3d = make_3d(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_2d = np.reshape(train_feat_3d, (94824,1287))\n",
    "test_feat_2d = np.reshape(test_feat_3d, (11005,1287))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94824, 1287), (11005, 1287))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat_2d.shape, test_feat_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\t 0.19045610334827315\n",
      "10\t 0.17690482467703664\n",
      "15\t 0.1920906933825468\n",
      "20\t 0.1774848404956499\n"
     ]
    }
   ],
   "source": [
    "#PERCEPTRON trainfeat2d, no scaling\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_feat_2d, words, test_size=0.2, random_state=12345)\n",
    "\n",
    "for passes in [5, 10, 15, 20]:\n",
    "    model = Perceptron(random_state=12345, max_iter=passes)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, model.predict(X_val))\n",
    "    print(\"{}\\t {}\".format(passes, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\t 0.2128130767202742\n",
      "10\t 0.20416556815185868\n",
      "15\t 0.2093857105193778\n",
      "20\t 0.2126021618771421\n"
     ]
    }
   ],
   "source": [
    "#PERCEPTRON trainfeat2d, with scale\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_feat_2d, words, test_size=0.2, random_state=12345)\n",
    "\n",
    "# Z-score the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "for passes in [5, 10, 15, 20]:\n",
    "    model = Perceptron(random_state=12345, max_iter=passes)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, model.predict(X_val))\n",
    "    print(\"{}\\t {}\".format(passes, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to work with the padded features\n",
    "def concat_summaries_3d(features, summaries):\n",
    "    \"\"\"function with as input: \n",
    "        -features\n",
    "        -list of summary functions to calculate for every 13 column over all the frames \n",
    "        (they will be concatenated next to each other) \"\"\"\n",
    "    \n",
    "    #create empty numpy matrix, with amount of features as rows, and (13 * amount of summary functions) for columns\n",
    "    new_feat = np.zeros((features.shape[0], 13*len(summaries)))  \n",
    "    \n",
    "    #fill the matrix\n",
    "    for i in range(features.shape[0]):\n",
    "        for j in range(13*len(summaries)):\n",
    "            coef_summaries = np.concatenate( [function(features[i], axis = 0) for function in summaries], axis = 0 )\n",
    "            new_feat[i,j] = coef_summaries[j]\n",
    "            \n",
    "    return new_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise the features to new feat min, max , std and mean statistics\n",
    "summaries = [np.mean, np.min, np.max, np.std]\n",
    "summaries1 = [np.mean, np.min, np.max]\n",
    "feat_meanminmaxstd3d = concat_summaries_3d(training_features, summaries)\n",
    "feat_meanminmax3d = concat_summaries_3d(training_features, summaries1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with summarised features and scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\t 0.2854205114684946\n",
      "10\t 0.26227260743474823\n",
      "15\t 0.29912997627208016\n",
      "20\t 0.29106248352227787\n"
     ]
    }
   ],
   "source": [
    "#feat_meanminmaxstd3d with scaling and perceptron\n",
    "X_train, X_val, y_train, y_val = train_test_split(feat_meanminmaxstd3d, words, test_size=0.2, random_state=12345)\n",
    "\n",
    "# Z-score the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "for passes in [5, 10, 15, 20]:\n",
    "    model = Perceptron(random_state=12345, max_iter=passes)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, model.predict(X_val))\n",
    "    print(\"{}\\t {}\".format(passes, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Logistic Regression with summarised features and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feat_meanminmaxstd3d with scaling and SGD Logistic\n",
    "X_train, X_val, y_train, y_val = train_test_split(feat_meanminmaxstd3d, words, test_size=0.2, random_state=12345)\n",
    "\n",
    "# Z-score the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "for passes in [5, 10, 15, 20]:\n",
    "    model = SGDClassifier(loss='log', random_state=12345, max_iter = passes)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(\"passes:{}\\t acc:{:.3}\".format(passes, accuracy_score(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC with min&max summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is with old summary engineered features, only meanminmax and no std\n",
    "X_train, X_val, y_train, y_val = train_test_split(feat_meanminmax3d, words, test_size=0.2, random_state=12345)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "svm = SVC(C = 3.0, random_state = 12345)\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_feat_3d = training_features\n",
    "#features with all the summaries: feat_all\n",
    "summaries = [np.mean, np.min, np.max, np.std, skew]\n",
    "feat_all = concat_summaries_3d(train_feat_3d, summaries) \n",
    "\n",
    "#for ablation create per summary a feature, so that we can easily stack them \n",
    "\n",
    "#only mean\n",
    "summaries = [np.mean]\n",
    "feat_mean_3d = concat_summaries_3d(train_feat_3d, summaries)\n",
    "#only min\n",
    "summaries = [np.min]\n",
    "feat_min_3d = concat_summaries_3d(train_feat_3d, summaries)\n",
    "#only max\n",
    "summaries = [np.max]\n",
    "feat_max_3d = concat_summaries_3d(train_feat_3d, summaries)\n",
    "#only std\n",
    "summaries = [np.std]\n",
    "feat_std_3d = concat_summaries_3d(train_feat_3d, summaries)\n",
    "#only skew\n",
    "summaries = [skew]\n",
    "feat_skew_3d = concat_summaries_3d(train_feat_3d, summaries)\n",
    "\n",
    "#create features stacked with all summaries except one of them\n",
    "feat_all_min_mean = np.hstack((feat_min_3d, feat_max_3d, feat_std_3d, feat_skew_3d)) # all summaries except mean\n",
    "feat_all_min_min = np.hstack((feat_mean_3d, feat_max_3d, feat_std_3d, feat_skew_3d)) # all summaries except min\n",
    "feat_all_min_max = np.hstack((feat_mean_3d, feat_min_3d, feat_std_3d, feat_skew_3d)) # all summaries except max\n",
    "feat_all_min_std = np.hstack((feat_mean_3d, feat_min_3d, feat_max_3d, feat_skew_3d)) # all summaries except std\n",
    "feat_all_min_skew = np.hstack((feat_mean_3d, feat_min_3d, feat_max_3d, feat_std_3d)) # all summaries except skew\n",
    "\n",
    "### Analysis\n",
    "## choose 25 passes for max_iter, good tradeoff for speed and acc for comparing\n",
    "\n",
    "# all the features:\n",
    "X_train, X_val, y_train, y_val = train_test_split(feat_all, words, test_size=0.2, random_state=12345)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "model = SGDClassifier(loss='log', random_state=12345, max_iter = 25)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "acc_all = accuracy_score(y_val, y_pred)\n",
    "print(\"All features acc:{:.3}\\t\".format( accuracy_score(y_val, y_pred)))\n",
    "\n",
    "### ablation analysis, calculate the relative accuracy drop when ablating each summary feature\n",
    "\n",
    "ablation_list = [feat_all_min_mean, feat_all_min_min, feat_all_min_max, feat_all_min_std, feat_all_min_skew]\n",
    "ablation_names = ['mean', 'min', 'max', 'std', 'skew']\n",
    "\n",
    "acc_drop = [] #create list we can fill and use for plot later\n",
    "\n",
    "for i, ablation in enumerate(ablation_list):\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(ablation, words, test_size=0.2, random_state=12345)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    model = SGDClassifier(loss='log', random_state=12345, max_iter = 25)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    rel_acc_drop = acc_all - accuracy_score(y_val, y_pred) #calculate relative acc drop\n",
    "    acc_drop.append(rel_acc_drop) # for the plot \n",
    "    name = ablation_names[i]\n",
    "    print(\"ablated feature: {}\\t acc: {:.3}\\t relative acc drop:{:.3}\".format(name, accuracy_score(y_val, y_pred), rel_acc_drop))\n",
    "    \n",
    "###plot figure\n",
    "x_pos = np.arange(len(ablation_names))\n",
    " \n",
    "plt.bar(x_pos, acc_drop) #create bars\n",
    "plt.xticks(x_pos, ablation_names) # create names under the bars\n",
    "\n",
    "plt.title('Feature Ablation Analysis') \n",
    "plt.ylabel('Relative Accuracy Drop')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('ablation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "pca = PCA(n_components=200, svd_solver='randomized')\n",
    "# Inputs\n",
    "pca.fit(train_feat_2d)\n",
    "X = pca.transform(train_feat_2d)\n",
    "# Output\n",
    "y = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN algortihm with 200 component (PCA)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors =1)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_val)\n",
    "print(confusion_matrix(y_val, pred))\n",
    "print(classification_report(y_val,pred))\n",
    "print(accuracy_score(y_val, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model - Neural Network(MLP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation & train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= train_feat_2d\n",
    "y = np.array(words, dtype='U10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Fit LabelEncoder with our list of classes\n",
    "label_encoder = LabelEncoder()\n",
    "#label_encoder.fit(y_train)\n",
    "label_encoder.fit(y)\n",
    "\n",
    "# Encode class values as integers\n",
    "#y_train = label_encoder.transform(y_train)\n",
    "y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Convert integers to dummy variables (one hot encoded)\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_val = np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, AlphaDropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Create a function to build model\n",
    "def build_model(input_dim,\n",
    "                   output_dim,\n",
    "                   n_dense=6,\n",
    "                   dense_neurons=512,\n",
    "                   activation='relu',\n",
    "                   dropout=AlphaDropout,\n",
    "                   dropout_rate=0.2,\n",
    "                   kernel_initializer='uniform',\n",
    "                   optimizer='adam'):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(dense_neurons, input_dim=input_dim,\n",
    "                        kernel_initializer=kernel_initializer))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "        for i in range(n_dense - 1):\n",
    "            model.add(Dense(dense_neurons, kernel_initializer=kernel_initializer))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(dropout(dropout_rate))\n",
    "\n",
    "        model.add(Dense(output_dim))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEfine model hyperparameters\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': X_train.shape[1],\n",
    "    'dense_neurons': 1024,\n",
    "    'output_dim': Y_train.shape[1],\n",
    "    'epochs': 20,\n",
    "    'batch_size': 512,\n",
    "    'verbose': 1,\n",
    "    #'validation_data': (X_val, Y_val),\n",
    "    'shuffle': True\n",
    "}\n",
    "clf = KerasClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "66376/66376 [==============================] - 70s 1ms/step - loss: 2.7556 - acc: 0.2515\n",
      "Epoch 2/20\n",
      "66376/66376 [==============================] - 69s 1ms/step - loss: 1.5295 - acc: 0.5548\n",
      "Epoch 3/20\n",
      "66376/66376 [==============================] - 63s 943us/step - loss: 1.1281 - acc: 0.6710\n",
      "Epoch 4/20\n",
      "66376/66376 [==============================] - 68s 1ms/step - loss: 0.9265 - acc: 0.7289\n",
      "Epoch 5/20\n",
      "66376/66376 [==============================] - 67s 1ms/step - loss: 0.7962 - acc: 0.7675\n",
      "Epoch 6/20\n",
      "66376/66376 [==============================] - 66s 998us/step - loss: 0.6985 - acc: 0.7958\n",
      "Epoch 7/20\n",
      "66376/66376 [==============================] - 69s 1ms/step - loss: 0.6256 - acc: 0.8163\n",
      "Epoch 8/20\n",
      "66376/66376 [==============================] - 66s 999us/step - loss: 0.5740 - acc: 0.8328\n",
      "Epoch 9/20\n",
      "66376/66376 [==============================] - 66s 987us/step - loss: 0.5275 - acc: 0.8469\n",
      "Epoch 10/20\n",
      "66376/66376 [==============================] - 70s 1ms/step - loss: 0.4925 - acc: 0.8557\n",
      "Epoch 11/20\n",
      "66376/66376 [==============================] - 71s 1ms/step - loss: 0.4610 - acc: 0.8664\n",
      "Epoch 12/20\n",
      "66376/66376 [==============================] - 75s 1ms/step - loss: 0.4330 - acc: 0.8755\n",
      "Epoch 13/20\n",
      "66376/66376 [==============================] - 70s 1ms/step - loss: 0.4004 - acc: 0.8828\n",
      "Epoch 14/20\n",
      "66376/66376 [==============================] - 70s 1ms/step - loss: 0.3766 - acc: 0.8901\n",
      "Epoch 15/20\n",
      "66376/66376 [==============================] - 78s 1ms/step - loss: 0.3610 - acc: 0.8953\n",
      "Epoch 16/20\n",
      "66376/66376 [==============================] - 74s 1ms/step - loss: 0.3394 - acc: 0.9023\n",
      "Epoch 17/20\n",
      "66376/66376 [==============================] - 74s 1ms/step - loss: 0.3273 - acc: 0.9069\n",
      "Epoch 18/20\n",
      "66376/66376 [==============================] - 89s 1ms/step - loss: 0.3136 - acc: 0.9099\n",
      "Epoch 19/20\n",
      "66376/66376 [==============================] - 101s 2ms/step - loss: 0.3087 - acc: 0.9121\n",
      "Epoch 20/20\n",
      "66376/66376 [==============================] - 87s 1ms/step - loss: 0.2925 - acc: 0.9173\n"
     ]
    }
   ],
   "source": [
    "hist = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28448/28448 [==============================] - 5s 160us/step\n",
      "0.8103205851950716\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_val, Y_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify with test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11005, 1287)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the test data shape\n",
    "test_feat_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11005/11005 [==============================] - 2s 195us/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction for test data\n",
    "X= test_feat_2d\n",
    "y_pred = clf.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the predictions from integer to labels back to prepare result.csv\n",
    "labels = label_encoder.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['bed', 'cat', 'seven', ..., 'five', 'left', 'backward'],\n",
       "       dtype='<U10'), (11005,), 35)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the labels, # of labels\n",
    "labels, labels.shape, len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "predictions =  pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0    bed\n",
       "1    cat\n",
       "2  seven\n",
       "3     no\n",
       "4    off"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4985e0c3784b700688c35818ba69f01b4fa3e8da.wav</td>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c3815898eb339919ab56249acae83cf566eb622d.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32c4865f292674cc904c5af503bc669c2dbd8843.wav</td>\n",
       "      <td>seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99600d05d1a861ef9771a7bd8eca0d5f444fce7a.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57ece62e925c94a7e6c54916caca1237467ad4d8.wav</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fc11b1ea9dfde4ebc927ff092b48a8ac86fddae3.wav</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15af624e3fce684f2a05a76902652d7388f1d912.wav</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42be94dcf7e9c40bd6ed5d45b9a41d6d87eb2890.wav</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ba326dca3f3e3b273a6c50f083f9eece3febe413.wav</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29d8a1c1f2d6c692aeb391c7efbc128cb20969ae.wav</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b124d416309908e839dca193a31d858f95e1fd2a.wav</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0d64cc8a1dcb1354f2b1b6fd8cbc03534e61c792.wav</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>f2378823f43e93208d07485150998d95009c56ec.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>533187f7e8f10c923fdc2a85d635286652490627.wav</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9c6c9b5b37efeaa1ed3474adc5f0903941ae4787.wav</td>\n",
       "      <td>marvin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>caa8dfc63b73bd6ca391d71c274949c04bee2abf.wav</td>\n",
       "      <td>eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b6f10d16a797b0d3c7655276c6a6a54913373229.wav</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43741f18d9757f593931ea9ef958f8ca4f7b030a.wav</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aaf3a3b233e045ccca2de3fcb8630f09e8db70e0.wav</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1e290890ff30b5e525993f652fb82ae3ca86c1dd.wav</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>af610012a51a2e7ecc4c1c839fbbe9f77d4e2d03.wav</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>947cc87dfcb9bfb64ea006fac7a1b0843af9b5b7.wav</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>a08d1b7997225c14405cb3d9babfaa37378b7c70.wav</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1f46f884f705d6ca447b15ffb2bf4a58ec595683.wav</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16475d16f5dca8ce70a24a21f0fe49dd354d02a5.wav</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8d8646f4c499201301e0a5843326ae977a83dea3.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1acda682c6d092aaa74c2f657087b3c3a921cccc.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4e1cd995952622b99d911e3cc545725103834fdf.wav</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>05a659e2ff9f34f6704fbcef653c642e1dc0cf4a.wav</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78d4e7f0d6d2b3418aa403c9e0b66716b64609d1.wav</td>\n",
       "      <td>learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>634ead779ce128c1890d90ffe4d29d066a6bfd5d.wav</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>8e69e433dd5332328d5f87f1e80a678b26b3cd2f.wav</td>\n",
       "      <td>eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>1093f5897ce74c28a9b2207b86e272d1a9f3964b.wav</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>2fae6a94e5816fd8f0207ab28663d1039890f9ad.wav</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>e9f0e17215cc2c3f651940c41e6a99f0a53063cd.wav</td>\n",
       "      <td>tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10980</th>\n",
       "      <td>9707227c19fd6cfcc84ec1b481a41ecc236e45e0.wav</td>\n",
       "      <td>seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10981</th>\n",
       "      <td>8cc554608b903e75273423f332647776d1223c51.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10982</th>\n",
       "      <td>fbc29dcc5a97fd8d6d9186111e561cc654373d87.wav</td>\n",
       "      <td>bird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10983</th>\n",
       "      <td>d60fb51d68c8a03d426df2c7f25fe70ab740774a.wav</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10984</th>\n",
       "      <td>6ae2640574331812af5ec2f7c1f301b05a52871c.wav</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10985</th>\n",
       "      <td>7bd040688e23ef492e572bab4971b86b09c8a169.wav</td>\n",
       "      <td>backward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10986</th>\n",
       "      <td>0b29230ccc3967bcb615d756dc0bc8196cd349a3.wav</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10987</th>\n",
       "      <td>b5ed1b55be98c8a1376c22bd3e791e9b229142ac.wav</td>\n",
       "      <td>seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10988</th>\n",
       "      <td>06e04537776c613e9f11e783617a29c0aa803dd1.wav</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10989</th>\n",
       "      <td>1a03ad681ca4486b1d3c17a1e3cd7f41aa5572f8.wav</td>\n",
       "      <td>eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10990</th>\n",
       "      <td>f3340434b4190f55702901e9b63aa3e9d5366db2.wav</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10991</th>\n",
       "      <td>2c2ec3806c71161352bce55f6caa3076a20f700a.wav</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10992</th>\n",
       "      <td>4022007eb409d8e3bffff81797ff2b4de3c4732e.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10993</th>\n",
       "      <td>3a8bbd9228ba5e0156f28713a1ef2d4307bff403.wav</td>\n",
       "      <td>nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10994</th>\n",
       "      <td>00ef5e841a0852d0250160dff66833ae623f58d2.wav</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>176edf041d8a5961845be15917532cda00753a70.wav</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>10bbad337a85d0962d4b7ee07cc4d6422e068592.wav</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>79c13cd6a175ff9df76eddab139bb394f38ea5bb.wav</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>d9d2a98b6ebdf57a08e3d86fe4d2c987d66c015f.wav</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>0ab7631cd0bd5a974c1a40c3e26756063e4a248b.wav</td>\n",
       "      <td>eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>21d4a6af950e635bef9d5a4af619ce3cc186206b.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11001</th>\n",
       "      <td>f8e8f270bbf9c7aeeceb961c5667b622f7baa208.wav</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11002</th>\n",
       "      <td>223a600994f93fcf869b2731a761b2358dc53c23.wav</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11003</th>\n",
       "      <td>6e39a17e99a29fa471a063738c821aa53c38c791.wav</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11004</th>\n",
       "      <td>8fb4f946c0aa002784366c02b41015efe084d432.wav</td>\n",
       "      <td>backward</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11005 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               path      word\n",
       "0      4985e0c3784b700688c35818ba69f01b4fa3e8da.wav       bed\n",
       "1      c3815898eb339919ab56249acae83cf566eb622d.wav       cat\n",
       "2      32c4865f292674cc904c5af503bc669c2dbd8843.wav     seven\n",
       "3      99600d05d1a861ef9771a7bd8eca0d5f444fce7a.wav        no\n",
       "4      57ece62e925c94a7e6c54916caca1237467ad4d8.wav       off\n",
       "5      fc11b1ea9dfde4ebc927ff092b48a8ac86fddae3.wav      stop\n",
       "6      15af624e3fce684f2a05a76902652d7388f1d912.wav      five\n",
       "7      42be94dcf7e9c40bd6ed5d45b9a41d6d87eb2890.wav     three\n",
       "8      ba326dca3f3e3b273a6c50f083f9eece3febe413.wav       one\n",
       "9      29d8a1c1f2d6c692aeb391c7efbc128cb20969ae.wav       two\n",
       "10     b124d416309908e839dca193a31d858f95e1fd2a.wav     three\n",
       "11     0d64cc8a1dcb1354f2b1b6fd8cbc03534e61c792.wav      five\n",
       "12     f2378823f43e93208d07485150998d95009c56ec.wav        no\n",
       "13     533187f7e8f10c923fdc2a85d635286652490627.wav        up\n",
       "14     9c6c9b5b37efeaa1ed3474adc5f0903941ae4787.wav    marvin\n",
       "15     caa8dfc63b73bd6ca391d71c274949c04bee2abf.wav     eight\n",
       "16     b6f10d16a797b0d3c7655276c6a6a54913373229.wav       off\n",
       "17     43741f18d9757f593931ea9ef958f8ca4f7b030a.wav      stop\n",
       "18     aaf3a3b233e045ccca2de3fcb8630f09e8db70e0.wav     house\n",
       "19     1e290890ff30b5e525993f652fb82ae3ca86c1dd.wav        on\n",
       "20     af610012a51a2e7ecc4c1c839fbbe9f77d4e2d03.wav     house\n",
       "21     947cc87dfcb9bfb64ea006fac7a1b0843af9b5b7.wav     three\n",
       "22     a08d1b7997225c14405cb3d9babfaa37378b7c70.wav     right\n",
       "23     1f46f884f705d6ca447b15ffb2bf4a58ec595683.wav     right\n",
       "24     16475d16f5dca8ce70a24a21f0fe49dd354d02a5.wav     three\n",
       "25     8d8646f4c499201301e0a5843326ae977a83dea3.wav        no\n",
       "26     1acda682c6d092aaa74c2f657087b3c3a921cccc.wav        no\n",
       "27     4e1cd995952622b99d911e3cc545725103834fdf.wav      stop\n",
       "28     05a659e2ff9f34f6704fbcef653c642e1dc0cf4a.wav      five\n",
       "29     78d4e7f0d6d2b3418aa403c9e0b66716b64609d1.wav     learn\n",
       "...                                             ...       ...\n",
       "10975  634ead779ce128c1890d90ffe4d29d066a6bfd5d.wav     three\n",
       "10976  8e69e433dd5332328d5f87f1e80a678b26b3cd2f.wav     eight\n",
       "10977  1093f5897ce74c28a9b2207b86e272d1a9f3964b.wav       dog\n",
       "10978  2fae6a94e5816fd8f0207ab28663d1039890f9ad.wav      stop\n",
       "10979  e9f0e17215cc2c3f651940c41e6a99f0a53063cd.wav      tree\n",
       "10980  9707227c19fd6cfcc84ec1b481a41ecc236e45e0.wav     seven\n",
       "10981  8cc554608b903e75273423f332647776d1223c51.wav        go\n",
       "10982  fbc29dcc5a97fd8d6d9186111e561cc654373d87.wav      bird\n",
       "10983  d60fb51d68c8a03d426df2c7f25fe70ab740774a.wav        on\n",
       "10984  6ae2640574331812af5ec2f7c1f301b05a52871c.wav        on\n",
       "10985  7bd040688e23ef492e572bab4971b86b09c8a169.wav  backward\n",
       "10986  0b29230ccc3967bcb615d756dc0bc8196cd349a3.wav       one\n",
       "10987  b5ed1b55be98c8a1376c22bd3e791e9b229142ac.wav     seven\n",
       "10988  06e04537776c613e9f11e783617a29c0aa803dd1.wav       off\n",
       "10989  1a03ad681ca4486b1d3c17a1e3cd7f41aa5572f8.wav     eight\n",
       "10990  f3340434b4190f55702901e9b63aa3e9d5366db2.wav       off\n",
       "10991  2c2ec3806c71161352bce55f6caa3076a20f700a.wav       yes\n",
       "10992  4022007eb409d8e3bffff81797ff2b4de3c4732e.wav       cat\n",
       "10993  3a8bbd9228ba5e0156f28713a1ef2d4307bff403.wav      nine\n",
       "10994  00ef5e841a0852d0250160dff66833ae623f58d2.wav      down\n",
       "10995  176edf041d8a5961845be15917532cda00753a70.wav      zero\n",
       "10996  10bbad337a85d0962d4b7ee07cc4d6422e068592.wav       one\n",
       "10997  79c13cd6a175ff9df76eddab139bb394f38ea5bb.wav     happy\n",
       "10998  d9d2a98b6ebdf57a08e3d86fe4d2c987d66c015f.wav       six\n",
       "10999  0ab7631cd0bd5a974c1a40c3e26756063e4a248b.wav     eight\n",
       "11000  21d4a6af950e635bef9d5a4af619ce3cc186206b.wav        go\n",
       "11001  f8e8f270bbf9c7aeeceb961c5667b622f7baa208.wav       off\n",
       "11002  223a600994f93fcf869b2731a761b2358dc53c23.wav      five\n",
       "11003  6e39a17e99a29fa471a063738c821aa53c38c791.wav      left\n",
       "11004  8fb4f946c0aa002784366c02b41015efe084d432.wav  backward\n",
       "\n",
       "[11005 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = test.join(predictions)\n",
    "result.columns = ['path', 'word']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results to the csv\n",
    "result.to_csv(\"result.csv\", sep=',', index= False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

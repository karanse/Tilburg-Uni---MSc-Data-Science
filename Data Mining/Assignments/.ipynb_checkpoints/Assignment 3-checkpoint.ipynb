{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANR: **924823**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "* Make a confusion matrix using the cats / dogs table.\n",
    "{'TP': 4, 'TN': 2, 'FP': 3, 'FN': 1}\n",
    "\n",
    "* Calculate Accuracy, Precision, Recall and F1 score.\n",
    "{'accuracy': 0.6, 'recall': 0.8, 'precision': 0.57, 'F1': 0.67}\n",
    "\n",
    "* How well does this classifier do on your prediction task (compared to a baseline)?\n",
    "Considering the majority baseline as 50%, these results show the prediction a bit better.\n",
    "\n",
    "* Think of a task where optimizing precision is important, and one where recall is important.\n",
    "\n",
    "Recall is important when we don’t want to miss out on any po- tential targets, regardless of how many errors we will make in the process, and misclassification comes at a low cost. Precision on the other hand is where misclassification does come at a high cost. As an example, misdiagnosing a patients may result so negatively so we require both high precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 4, 'TN': 2, 'FP': 3, 'FN': 1}\n"
     ]
    }
   ],
   "source": [
    "TP = 4\n",
    "TN = 2\n",
    "FP = 3\n",
    "FN = 1\n",
    "\n",
    "conf_m = {'TP':TP, 'TN':TN, 'FP':FP,'FN':FN}\n",
    "print(conf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 0.8 0.57 0.67\n",
      "{'accuracy': 0.6, 'recall': 0.8, 'precision': 0.57, 'F1': 0.67}\n"
     ]
    }
   ],
   "source": [
    "accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP +FP)\n",
    "\n",
    "F1 = 2 *((precision * recall)/(precision + recall))\n",
    "\n",
    "dic = {'accuracy':accuracy, 'recall': recall, 'precision':round(precision,2), 'F1':round(F1,2)}\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Try to solve the mistakes in the methods for the cases below given what you know of earlier mentioned concepts, and the description of the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Case 1** — We sampled about 20% of the total data. We apply stan- dard Linear Regression and get an error of 0.05. We’re happy with the result and report it as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Case 2** — We use all of the data, but split off 10% for testing our model. We start applying k-NN and tuning k manually until we get the highest accuracy score on the train set (k=50, accu- racy=0.80). We then apply it to the test set and get a score of 0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

## Find potential outliers in all numeric columns:
lapply(tests[ , -c(1, 2)], FUN = madOutliers)
###--------------------------------------------------------------------------###
## Assign function arguments to demonstrate its internals:
data  <- tests[ , -c(1, 2)]
prob  <- 0.99
ratio <- 0.75
## Define a function to implement a robust version of Mahalanobis distance
## using MCD estimation:
mcdMahalanobis <- function(data, prob, ratio = 0.75) {
## Compute the MCD estimates of the mean and covariance matrix:
stats <- cov.mcd(data, quantile.used = floor(ratio * nrow(data)))
## Compute robust squared Mahalanobis distances
md <- mahalanobis(x = data, center = stats$center, cov = stats$cov)
## Find the cutoff value:
crit <- qchisq(prob, df = ncol(data))
## Return row indices of flagged observations:
which(md > crit)
}
rm(list = c("data", "prob", "ratio", "stats", "md", "crit"))# Clean up
## Flag potential multivariate outliers:
mcdMahalanobis(data = tests[ , -c(1, 2)], prob = 0.99)
## Fit a simple linear regression model:
lmOut1 <- lm(qsec ~ hp, data = cars)
## Evaluating the fitted models symbol doesn't give us much:
lmOut1
## Summarize the model:
summary(lmOut1)
## Look at the contents of the fitted model object and its summary:
ls(lmOut1)
plot(data=cars, x=qsec, y=hp)
View(cars)
plot(data=cars, x=`qsec`, y=hp)
## Look at the contents of the fitted model object and its summary:
ls(lmOut1)
plot(data=cars, qsec~hp)
ls(lmOut1)
ls(summary(lmOut1))
## Fit a multiple linear regression model:
lmOut2 <- lm(qsec ~ hp + wt, data = cars)
s2 <- summary(lmOut2)
s2
## Extract R^2:
s2$r.squared
## Extract coefficients:
coef(lmOut2)
res2 <- resid(lmOut2)
res2
sig2 <- s2$sigma
sig2
sig2Sq <- sig2^2
sig2Sq
ls(lmOut2)
sig2SqM <- sum(res2^2) / lmOut2$df.residual
sig2Sq - sig2SqM
## Extract fitted values (two ways):
yHat2.1 <- fitted(lmOut2)
yHat2.2 <- predict(lmOut2)
yHat2.1
yHat2.2
## Compute confidence intervals:
confint(lmOut2)
confint(lmOut2, parm = "hp")
confint(lmOut2, parm = c("(Intercept)", "wt"), level = 0.99)
getwd()
### Title:    Stats & Methods Lab 2 Demonstration Script
### Author:   Kyle M. Lang
### Created:  2017-SEP-08
### Modified: 2018-SEP-11
###--------------------------------------------------------------------------###
### Preliminaries ###
install.packages("MLmetrics", repos = "http://cloud.r-project.org")
rm(list = ls(all = TRUE)) # Clear workspace
library(MASS)      # We'll need this for robust stats
library(MLmetrics) # We'll need this for MSEs
setwd("") # Let's all set our working directory to the correct place
###--------------------------------------------------------------------------###
### Data I/O ###
dataDir <- "./data/"
fn1     <- "tests.rds"
fn2     <- "mtcars.rds"
tests <- readRDS(paste0(dataDir, fn1))
cars  <- readRDS(paste0(dataDir, fn2))
###--------------------------------------------------------------------------###
### Outlier Analysis ###
## Compute the statistics underlying a boxplot:
boxplot.stats(tests$SATV)
## Assign function argument to demonstrate its internals:
x <- tests$SATV
## Define a function to implement the boxplot method:
bpOutliers <- function(x) {
## Compute inner and outer fences:
iFen <- boxplot.stats(x, coef = 1.5)$stats[c(1, 5)]
oFen <- boxplot.stats(x, coef = 3.0)$stats[c(1, 5)]
## Return the row indices of flagged 'possible' and 'probable' outliers:
list(possible = which(x < iFen[1] | x > iFen[2]),
probable = which(x < oFen[1] | x > oFen[2])
)
}
rm(list = c("x", "iFen", "oFen"))# Clean up
## Flag potential outliers in all numeric columns:
lapply(tests[ , -c(1, 2)], FUN = bpOutliers)
###--------------------------------------------------------------------------###
## Assign function arguments to demonstrate its internals:
x     <- tests$SATV
cut   <- 2.5
na.rm <- TRUE
## Define a function to implement the MAD method:
madOutliers <- function(x, cut = 2.5, na.rm = TRUE) {
## Compute the median and MAD of x:
mX   <- median(x, na.rm = na.rm)
madX <- mad(x, na.rm = na.rm)
## Return row indices of observations for which |T_MAD| > cut:
which(abs(x - mX) / madX > cut)
}
rm(list = c("x", "cut", "na.rm", "mX", "madX"))# Clean up
## Find potential outliers in all numeric columns:
lapply(tests[ , -c(1, 2)], FUN = madOutliers)
###--------------------------------------------------------------------------###
## Assign function arguments to demonstrate its internals:
data  <- tests[ , -c(1, 2)]
prob  <- 0.99
ratio <- 0.75
## Define a function to implement a robust version of Mahalanobis distance
## using MCD estimation:
mcdMahalanobis <- function(data, prob, ratio = 0.75) {
## Compute the MCD estimates of the mean and covariance matrix:
stats <- cov.mcd(data, quantile.used = floor(ratio * nrow(data)))
## Compute robust squared Mahalanobis distances
md <- mahalanobis(x = data, center = stats$center, cov = stats$cov)
## Find the cutoff value:
crit <- qchisq(prob, df = ncol(data))
## Return row indices of flagged observations:
which(md > crit)
}
rm(list = c("data", "prob", "ratio", "stats", "md", "crit"))# Clean up
## Flag potential multivariate outliers:
mcdMahalanobis(data = tests[ , -c(1, 2)], prob = 0.99)
###--------------------------------------------------------------------------###
### Linear regression ###
## Fit a simple linear regression model:
lmOut1 <- lm(qsec ~ hp, data = cars)
## Evaluating the fitted models symbol doesn't give us much:
lmOut1
## Summarize the model:
summary(lmOut1)
plot(data=cars, qsec~hp)
## Look at the contents of the fitted model object and its summary:
ls(lmOut1)
ls(summary(lmOut1))
## Fit a multiple linear regression model:
lmOut2 <- lm(qsec ~ hp + wt, data = cars)
## Summarize the model:
s2 <- summary(lmOut2)
s2
## Extract R^2:
s2$r.squared
## Extract F-stat:
s2$fstatistic
## Extract coefficients:
coef(lmOut2)
## Extract residuals:
res2 <- resid(lmOut2)
res2
## Extract residual standard deviation/variance:
sig2 <- s2$sigma
sig2
sig2Sq <- sig2^2
sig2Sq
## Calculate the residual variance manually and compare:
sig2SqM <- sum(res2^2) / lmOut2$df.residual
sig2Sq - sig2SqM
## Extract fitted values (two ways):
yHat2.1 <- fitted(lmOut2)
yHat2.2 <- predict(lmOut2)
yHat2.1
yHat2.2
## Compare:
yHat2.1 - yHat2.2
## Compute confidence intervals:
confint(lmOut2)
confint(lmOut2, parm = "hp")
confint(lmOut2, parm = c("(Intercept)", "wt"), level = 0.99)
## Mean-center the horsepower variable (two ways):
hpMC1 <- cars$hp - mean(cars$hp)
hpMC2 <- scale(cars$hp, scale = FALSE)
hpMC1 - hpMC2
## Use mean-centered hp and wt in a model:
head(cars)
cars[ , c("hpMC", "wtMC")] <- scale(cars[ , c("hp", "wt")], scale = FALSE)
head(cars)
lmOut3 <- lm(qsec ~ hpMC + wtMC, data = cars)
summary(lmOut3)
###--------------------------------------------------------------------------###
### Model comparison ###
## Change in R^2:
s2$r.squared - summary(lmOut1)$r.squared
## Significant increase in R^2?
anova(lmOut1, lmOut2)
## Compare MSE values:
mse1 <- MSE(y_pred = predict(lmOut1), y_true = cars$qsec)
mse2 <- MSE(y_pred = predict(lmOut2), y_true = cars$qsec)
mse1
mse2
###--------------------------------------------------------------------------###
## Mean-center the horsepower variable (two ways):
hpMC1 <- cars$hp - mean(cars$hp)
hpMC2 <- scale(cars$hp, scale = FALSE)
hpMC1 - hpMC2
## Use mean-centered hp and wt in a model:
head(cars)
## Mean-center the horsepower variable (two ways):
hpMC1 <- cars$hp - mean(cars$hp)
hpMC2 <- scale(cars$hp, scale = FALSE)
hpMC1 - hpMC2
## Use mean-centered hp and wt in a model:
head(cars)
cars[ , c("hpMC", "wtMC")] <- scale(cars[ , c("hp", "wt")], scale = FALSE)
head(cars)
lmOut3 <- lm(qsec ~ hpMC + wtMC, data = cars)
summary(lmOut3)
## Change in R^2:
s2$r.squared - summary(lmOut1)$r.squared
## Significant increase in R^2?
anova(lmOut1, lmOut2)
mse1 <- MSE(y_pred = predict(lmOut1), y_true = cars$qsec)
mse2 <- MSE(y_pred = predict(lmOut2), y_true = cars$qsec)
mse1
mse2
install.packages("MLmetrics", repos = "http://cloud.r-project.org")
rm(list = ls(all = TRUE)) # Clear workspace
library(MASS)      # We'll need this for robust stats
library(MLmetrics) # We'll need this for MSEs
dataDir <- "./data/"
fn1     <- "tests.rds"
fn2     <- "mtcars.rds"
tests <- readRDS(paste0(dataDir, fn1))
cars  <- readRDS(paste0(dataDir, fn2))
## Compute the statistics underlying a boxplot:
boxplot.stats(tests$SATV)
oFen <- boxplot.stats(x, coef = 3.0)$stats[c(1, 5)]
## Define a function to implement the boxplot method:
bpOutliers <- function(x) {
## Compute inner and outer fences:
iFen <- boxplot.stats(x, coef = 1.5)$stats[c(1, 5)]
oFen <- boxplot.stats(x, coef = 3.0)$stats[c(1, 5)]
bpOutliers <- function(x) {
## Compute inner and outer fences:
iFen <- boxplot.stats(x, coef = 1.5)$stats[c(1, 5)]
oFen <- boxplot.stats(x, coef = 3.0)$stats[c(1, 5)]
## Return the row indices of flagged 'possible' and 'probable' outliers:
list(possible = which(x < iFen[1] | x > iFen[2]),
probable = which(x < oFen[1] | x > oFen[2])
)
}
rm(list = c("x", "iFen", "oFen"))# Clean up
## Flag potential outliers in all numeric columns:
lapply(tests[ , -c(1, 2)], FUN = bpOutliers)
## 1) Use the "install.packages" function to install the "MLmetrics" package.
library(MLmetrics)
## 2) Use the "library" function to load the "MASS" and "MLmetrics" packages.
library(MASS)
dataDir <- "./data/"
airqual    <- "airQual.rds"
aqdata <- readRDS(paste0(dataDir, airqual))
bpOutliers <- function(x) {
## Compute inner and outer fences:
iFen <- boxplot.stats(x, coef = 1.5)$stats[c(1, 5)]
oFen <- boxplot.stats(x, coef = 3.0)$stats[c(1, 5)]
## Return the row indices of flagged 'possible' and 'probable' outliers:
list(possible = which(x < iFen[1] | x > iFen[2]),
probable = which(x < oFen[1] | x > oFen[2])
)
}
lapply(aqdata[ , -c(1, 2)], FUN = bpOutliers)
View(aqdata)
#-->No
## 1d) Which, if any, observations were possible outliers on "Ozone"?
bpOutliers(aqdata$Ozone)
lapply(aqdata[ , ], FUN = bpOutliers)
madOutliers <- function(x, cut = 2.5, na.rm = TRUE) {
## Compute the median and MAD of x:
mX   <- median(x, na.rm = na.rm)
madX <- mad(x, na.rm = na.rm)
## Return row indices of observations for which |T_MAD| > cut:
which(abs(x - mX) / madX > cut)
}
rm(list = c("x", "cut", "na.rm", "mX", "madX"))#
madOutliers <- function(x, cut = 2.5, na.rm = TRUE) {
## Compute the median and MAD of x:
mX   <- median(x, na.rm = na.rm)
madX <- mad(x, na.rm = na.rm)
## Return row indices of observations for which |T_MAD| > cut:
which(abs(x - mX) / madX > cut)
}
rm(list = c("x", "cut", "na.rm", "mX", "madX"))# Clean up
## Find potential outliers in all numeric columns:
lapply(aqdata[ , ], FUN = madOutliers)
dataDir <- "./data/"
airqual    <- "airQual.rds"
aqdata <- readRDS(paste0(dataDir, airqual))
mcdMahalanobis <- function(data, prob, ratio = 0.75) {
## Compute the MCD estimates of the mean and covariance matrix:
stats <- cov.mcd(data, quantile.used = floor(ratio * nrow(data)))
## Compute robust squared Mahalanobis distances
md <- mahalanobis(x = data, center = stats$center, cov = stats$cov)
## Find the cutoff value:
crit <- qchisq(prob, df = ncol(data))
## Return row indices of flagged observations:
which(md > crit)
}
rm(list = c("data", "prob", "ratio", "stats", "md", "crit"))
mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.99)
## 3b) Which, if any, observations are flagged as multivariate outliers when
##     using 75% of the sample for the MCD estimation and using a probability of
##     0.999 for the cutoff value?
mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.999)
## 3c) Which, if any, observations are flagged as multivariate outliers when
##     using 50% of the sample for the MCD estimation and using a probability of
##     0.99 for the cutoff value
mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.99, ratio = 0.50)
sum(!is.na(mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.99, ratio = 0.50)))
sum(!is.na(mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.999)))
sum(!is.na(mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.99)))
## 3d) Which, if any, observations are flagged as multivariate outliers when
##     using 50% of the sample for the MCD estimation and using a probability of
##     0.999 for the cutoff value?
mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.999, ratio = 0.50)
sum(!is.na(mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.999, ratio = 0.50)))
mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.99)
## 3a) Which, if any, observations are flagged as multivariate outliers when
##     using 75% of the sample for the MCD estimation and using a probability of
##     0.99 for the cutoff value?
# yes, 17 values
## 3b) Which, if any, observations are flagged as multivariate outliers when
##     using 75% of the sample for the MCD estimation and using a probability of
##     0.999 for the cutoff value?
mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.999)
mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.99) == mcdMahalanobis(data = aqdata[ , -c(5,6)], prob = 0.999)
d2 <- data("longley")
## 1) Regress "GNP" onto "Year."
out1 <- lm(GNP ~ Year, data = d2)
View(longley)
d2 <- data("longley")
## 1) Regress "GNP" onto "Year."
out1 <- lm(GNP ~ Year, data = longley)
## 2) What proportion of variability in "GNP" is explained by "Year?"
summary(out1)
## 2) What proportion of variability in "GNP" is explained by "Year?"
s1 <- summary(out1)
s1$r.squared
rount(s1$r.squared,3)
round(s1$r.squared,3)
## 3a) What is the slope of "Year" on "GNP?"
s2 <- summary(out1)
s2$coefficients
## Fit a simple linear regression model:
lmOut1 <- lm(qsec ~ hp, data = cars)
dataDir <- "./data/"
fn1     <- "tests.rds"
fn2     <- "mtcars.rds"
tests <- readRDS(paste0(dataDir, fn1))
cars  <- readRDS(paste0(dataDir, fn2))
## Fit a simple linear regression model:
lmOut1 <- lm(qsec ~ hp, data = cars)
## Evaluating the fitted models symbol doesn't give us much:
lmOut1
summary(lmOut1)
plot(data=cars, qsec~hp)
ls(lmOut1)
ls(summary(lmOut1))
## Fit a multiple linear regression model:
lmOut2 <- lm(qsec ~ hp + wt, data = cars)
s2 <- summary(lmOut2)
s2
## Extract R^2:
s2$r.squared
## Extract F-stat:
s2$fstatistic
## Extract coefficients:
coef(lmOut2)
res2 <- resid(lmOut2)
res2
sig2 <- s2$sigma
sig2
sig2Sq <- sig2^2
sig2Sq
## 4) What is the residual variance for the model estimated in Step 1?
std < - s1$sigma
## 4) What is the residual variance for the model estimated in Step 1?
std < - s1$sigma
## 4) What is the residual variance for the model estimated in Step 1?
s1_sig< - s1$sigma
## 2) What proportion of variability in "GNP" is explained by "Year?"
s1 <- summary(out1)
## 4) What is the residual variance for the model estimated in Step 1?
s1_sig< - s1$sigma
## 4) What is the residual variance for the model estimated in Step 1?
s1_sig <- s1$sigma
s1_var <- s1_sig^2
s1_var
round(s1_var,2)
sig2SqM <- sum(res2^2) / lmOut2$df.residual
sig2Sq - sig2SqM
## Extract fitted values (two ways):
yHat2.1 <- fitted(lmOut2)
yHat2.2 <- predict(lmOut2)
yHat2.1
yHat2.2
confint(out1)
confint(out1, parm = "hp")
confint(lmOut2)
confint(lmOut2, parm = "hp")
confint(lmOut2, parm = c("(Intercept)", "wt"), level = 0.99)
View(cars)
confint(out1)
## 5b) What is the 99% confidence interval (CI) for the intercept of the model
##     estimated in Step 1?
confint(out1, parm = c("(Intercept)", "Year"), level = 0.99)
round(( 22.39144- 19.16544 ),2)
round((-37071.14087 + -43376.37157 ),2)
## 6) Regress "GNP" onto "Population."
out2 <- lm(GNP ~ Population, data = longley)
summary(out2)
s2 <-summary(out2)
## 7a) What is the standard error of the slope of "Population" on "GNP?"
s2
round(0.5086,3)
## 1) Regress "GNP" onto "Year" and "Population."
out2 <- lm(GNP ~ Year + Population, data = longley)
## 1) Regress "GNP" onto "Year" and "Population."
out3 <- lm(GNP ~ Year + Population, data = longley)
## 2) Is there a significant effect (alpha = 0.05) of "Population" on "GNP"
##    after controlling for "Year?"
summary(out3)
## 3) Regress "GNP" onto "Year" and "Employed."
out4 <- lm(GNP ~ Year + Employed, data=longley)
summary(out4)
round(8.419,2)
summary(out1)
summary(out3)
## 1a) What is the difference in R-squared between the simple linear regression
##     of "GNP" onto "Year" and the multiple linear regression of "GNP" onto
##     "Year" and "Population?"
s1 <- summary(out1)
s3 <- summary(out3)
s1$r.squared - s3$r.squared
round(( s3$r.squared-s1$r.squared),6)
## 1b) Is this increase in R-squared significantly different from zero at the
##     alpha = 0.05 level?
anova(out1,out3)
## Change in R^2:
s2$r.squared - summary(lmOut1)$r.squared
## Significant increase in R^2?
anova(lmOut1, lmOut2)
summary(out4)
## 2a) What is the MSE for the model that regresses "GNP" onto "Year" and
##     "Employed?"
mse4 <- MSE(y_pred = predict(out4), y_true = longley$GNP)
mse4
## 2b) What is the MSE for the model that regresses "GNP" onto "Year" and
##     "Unemployed?"
out5 <- lm(GNP ~ Year + Unemployed, data = longley)
## 2b) What is the MSE for the model that regresses "GNP" onto "Year" and
##     "Unemployed?"
out5 <- lm(GNP ~ Year + Unemployed, data = longley)mse5 <- MSE(y_pred = predict(out5), y_true = longley$GNP)
out5 <- lm(GNP ~ Year + Unemployed, data = longley)
mse5 <- MSE(y_pred = predict(out5), y_true = longley$GNP)
mse5
round(25.40593,2)
## 1b) Is this increase in R-squared significantly different from zero at the
##     alpha = 0.05 level?
anova(out1,out3) #---> No, greated than alpha
round(0.5397,3)
aqdata <- readRDS(paste0(dataDir, airqual))
bpOutliers <- function(x) {
## Compute inner and outer fences:
iFen <- boxplot.stats(x, coef = 1.5)$stats[c(1, 5)]
oFen <- boxplot.stats(x, coef = 3.0)$stats[c(1, 5)]
## Return the row indices of flagged 'possible' and 'probable' outliers:
list(possible = which(x < iFen[1] | x > iFen[2]),
probable = which(x < oFen[1] | x > oFen[2])
)
}
lapply(aqdata[ , ], FUN = bpOutliers)
confint(out1)
out4 <- lm(GNP ~ Year + Employed, data=longley)
summary(out4)
## 7a) What is the standard error of the slope of "Population" on "GNP?" ---> 13.7
s2
round(0.5086,3)
## 1a) What is the difference in R-squared between the simple linear regression
##     of "GNP" onto "Year" and the multiple linear regression of "GNP" onto
##     "Year" and "Population?"
s1 <- summary(out1)
s1
s3
s1$r.squared - s3$r.squared
s3$r.squared -s1$r.squared
round(0.0002793063,6)
out5 <- lm(GNP ~ Year + Unemployed, data = longley)
mse5 <- MSE(y_pred = predict(out5), y_true = longley$GNP)
mse5
round(25.40593,2)
## 1b) Is this increase in R-squared significantly different from zero at the
##     alpha = 0.05 level?
anova(out1,out3) #---> No, greated than alpha
s1_sig <- s1$sigma
s1_var <- s1_sig^2
round(s1_var,2)
s1 <- summary(out1)
round(s1$r.squared,3)
